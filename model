import torch.nn as nn
from sklearn.metrics import mean_squared_error
import numpy as np
from transformers import BertConfig
from transformers.models.bert.modeling_bert import BertSelfOutput, BertIntermediate, BertOutput
import pygmt
import xarray as xr
import torch
from torch.cuda.amp import autocast, GradScaler
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class AttentionPooling(nn.Module):
    def __init__(self, config):
        super(AttentionPooling, self).__init__()
        self.config = config
        self.att_fc1 = nn.Linear(config.hidden_size, config.hidden_size)
        self.att_fc2 = nn.Linear(config.hidden_size, 1)
        self.apply(self.init_weights)
    def init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
    def forward(self, x, attn_mask=None):
        bz = x.shape[0]
        e = self.att_fc1(x)
        e = nn.Tanh()(e)
        alpha = self.att_fc2(e)
        alpha = torch.exp(alpha)
        if attn_mask is not None:
            alpha = alpha * attn_mask.unsqueeze(-1)
        alpha = alpha / (torch.sum(alpha, dim=1, keepdim=True) + 1e-8)
        x = torch.bmm(x.permute(0, 2, 1), alpha)
        x = torch.reshape(x, (bz, -1))
        return x

class FastSelfAttention(nn.Module):
    def __init__(self, config):
        super(FastSelfAttention, self).__init__()
        self.config = config
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError(
                f"The hidden size ({config.hidden_size}) is not a multiple of the number of attention "
                f"heads ({config.num_attention_heads})")
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.num_attention_heads = config.num_attention_heads
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.input_dim = config.hidden_size
        self.query_proj = nn.Linear(self.input_dim, self.all_head_size)
        self.key_proj = nn.Linear(self.input_dim, self.all_head_size)
        self.value_proj = nn.Linear(self.input_dim, self.all_head_size)
        self.global_q_att = nn.Linear(self.attention_head_size, 1)
        self.qk_att = nn.Linear(self.attention_head_size, 1)
        self.kv_att = nn.Linear(self.attention_head_size, 1)
        self.output_proj = nn.Linear(self.all_head_size, self.all_head_size)
        self.softmax = nn.Softmax(dim=2)
        self.apply(self.init_weights)
    def init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)
    def forward(self, hidden_states, attention_mask):
        batch_size, seq_len, _ = hidden_states.shape
        q = self.query_proj(hidden_states)
        k = self.key_proj(hidden_states)
        v = self.value_proj(hidden_states)
        q_heads = self.transpose_for_scores(q)
        k_heads = self.transpose_for_scores(k)
        v_heads = self.transpose_for_scores(v)
        q_scores = self.global_q_att(q_heads).squeeze(-1)
        q_scores += attention_mask
        q_weights = self.softmax(q_scores).unsqueeze(-1)
        global_q = torch.sum(q_heads * q_weights, dim=2)
        global_q_expanded = global_q.unsqueeze(2).expand(-1, -1, seq_len, -1)
        qk_scores = self.qk_att(global_q_expanded * k_heads).squeeze(-1)
        qk_scores += attention_mask
        qk_weights = self.softmax(qk_scores).unsqueeze(-1)
        global_k = torch.sum(k_heads * qk_weights, dim=2)
        global_k_expanded = global_k.unsqueeze(2).expand(-1, -1, seq_len, -1)
        kv_scores = self.kv_att(global_k_expanded * v_heads).squeeze(-1)
        kv_scores += attention_mask
        kv_weights = self.softmax(kv_scores).unsqueeze(-1)
        global_v = torch.sum(v_heads * kv_weights, dim=2)
        global_v_combined = global_v.permute(0, 2, 1).contiguous()
        global_v_combined = global_v_combined.view(batch_size, self.all_head_size)
        global_v_combined = global_v_combined.unsqueeze(1).expand(-1, seq_len, -1)
        output = self.output_proj(global_v_combined)
        output = output + hidden_states
        return output

class FastAttention(nn.Module):
    def __init__(self, config):
        super(FastAttention, self).__init__()
        self.self = FastSelfAttention(config)
        self.output = BertSelfOutput(config)
    def forward(self, input_tensor, attention_mask):
        self_output = self.self(input_tensor, attention_mask)
        attention_output = self.output(self_output, input_tensor[:, :1, :])
        return attention_output

class FastformerLayer(nn.Module):
    def __init__(self, config):
        super(FastformerLayer, self).__init__()
        self.attention = FastAttention(config)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)
    def forward(self, hidden_states, attention_mask):
        attention_output = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output

class FastformerEncoder(nn.Module):
    def __init__(self, config, pooler_count=1):
        super(FastformerEncoder, self).__init__()
        self.config = config
        self.num_attention_heads = config.num_attention_heads
        self.encoders = nn.ModuleList([FastformerLayer(config) for _ in range(config.num_hidden_layers)])
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.poolers = nn.ModuleList()
        if config.pooler_type == 'weightpooler':
            for _ in range(pooler_count):
                self.poolers.append(AttentionPooling(config))
        print(f"This model has {len(self.poolers)} poolers.")
        self.apply(self.init_weights)
    def init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if isinstance(module, nn.Embedding) and module.padding_idx is not None:
                with torch.no_grad():
                    module.weight[module.padding_idx].fill_(0)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
    def forward(self, input_embs, attention_mask, pooler_index=0):
        extended_attention_mask = attention_mask.unsqueeze(1)
        extended_attention_mask = extended_attention_mask.expand(-1, self.num_attention_heads, -1)
        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        batch_size, seq_length, emb_dim = input_embs.shape
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_embs.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        position_embeddings = self.position_embeddings(position_ids)
        embeddings = input_embs + position_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        all_hidden_states = [embeddings]
        for layer_module in self.encoders:
            layer_outputs = layer_module(all_hidden_states[-1], extended_attention_mask)
            all_hidden_states.append(layer_outputs)
        assert len(self.poolers) > pooler_index, f"Pooler index {pooler_index} out of range"
        output = self.poolers[pooler_index](all_hidden_states[-1], attention_mask)
        return output

class Model(torch.nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(7, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 48, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(48),
            nn.Conv2d(48, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
        )
        self.cnn_to_fastformer = nn.Conv2d(64, config.hidden_size, kernel_size=1, padding=0)
        self.fastformer_model = FastformerEncoder(config)
        self.attention = AttentionPooling(config)
        self.dense_linear = nn.Linear(config.hidden_size, 1)
        self.criterion = nn.SmoothL1Loss(beta=0.1)
    def forward(self, input_tensor, targets):
        x = self.conv(input_tensor)
        x = self.cnn_to_fastformer(x)
        batch_size = x.shape[0]
        hidden_size = x.shape[1]
        H, W = x.shape[2], x.shape[3]
        seq_len = H * W
        x = x.permute(0, 2, 3, 1).reshape(batch_size, seq_len, hidden_size)
        x = nn.Dropout(0.2)(x)
        mask = torch.ones((batch_size, seq_len), device=x.device)
        text_vec = self.fastformer_model(x, mask)
        score = self.dense_linear(text_vec)
        if targets is not None:
            loss = self.criterion(score.squeeze(), targets.squeeze())
            return loss, score
        return score

control = pd.read_csv('', sep='\t', header=None)
check = pd.read_csv('', sep='\t', header=None)
free = pygmt.grdcut(grid="", region="")
free_15s = pygmt.grdsample(grid=free, spacing="15s")
vgg = pygmt.grdcut(grid="", region="")
vgg_15s = pygmt.grdsample(grid=vgg, spacing="15s")
nvd = pygmt.grdcut(grid="", region="")
nvd_15s = pygmt.grdsample(grid=nvd, spacing="15s")
evd = pygmt.grdcut(grid="", region="")
evd_15s = pygmt.grdsample(grid=evd, spacing="15s")
topo_25_1 = pygmt.grdcut(grid="", region="")
topo_25_1_15s = pygmt.grdsample(grid=topo_25_1, spacing="15s")

con_lat_index = free_15s.indexes["lat"].get_indexer(control.values[:, 1], method="nearest")
con_lon_index = free_15s.indexes["lon"].get_indexer(control.values[:, 0], method="nearest")
che_lat_index = free_15s.indexes["lat"].get_indexer(check.values[:, 1], method="nearest")
che_lon_index = free_15s.indexes["lon"].get_indexer(check.values[:, 0], method="nearest")

x_train_list = []
y_train_list = []
for j in range(len(control)):
    center_lat = con_lat_index[j]
    center_lon = con_lon_index[j]
    start_lat = max(0, center_lat - 8)
    end_lat = min(len(free_15s.lat), center_lat + 8)
    start_lon = max(0, center_lon - 8)
    end_lon = min(len(free_15s.lon), center_lon + 8)
    free_values = free_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    vgg_values = vgg_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    nvd_values = nvd_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    evd_values = evd_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    topo_values = topo_25_1_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    lat_slice = free_15s.lat[start_lat:end_lat]
    lon_slice = free_15s.lon[start_lon:end_lon]
    lon_grid, lat_grid = np.meshgrid(lon_slice, lat_slice)
    dlon = lon_grid - control.values[j, 0]
    dlat = lat_grid - control.values[j, 1]
    center_idx = 8
    free_center = free_values[center_idx, center_idx]
    free_diff = free_values - free_center
    vgg_center = vgg_values[center_idx, center_idx]
    vgg_diff = vgg_values - vgg_center
    nvd_center = nvd_values[center_idx, center_idx]
    nvd_diff = nvd_values - nvd_center
    evd_center = evd_values[center_idx, center_idx]
    evd_diff = evd_values - evd_center
    topo_center = topo_values[center_idx, center_idx]
    topo_diff = topo_values - topo_center
    data_temp = np.array([dlon, dlat, free_diff, vgg_diff, nvd_diff, evd_diff, topo_diff])
    if data_temp.shape != (7, 16, 16):
        continue
    x_train_list.append(data_temp)
    y_raw = control.values[j, 2]
    y_diff = topo_center - y_raw
    y_train_list.append(y_diff)
    del lat_slice, lon_slice, lon_grid, lat_grid

x_train = np.array(x_train_list)
y_train = np.array(y_train_list)

m, c, h, w = x_train.shape
scalers = []
x_train_reshaped = x_train.reshape(m, c, h * w)
for i in range(c):
    scaler = MinMaxScaler()
    feature_flat = x_train_reshaped[:, i, :].reshape(-1, 1)
    feature_scaled = scaler.fit_transform(feature_flat)
    x_train_reshaped[:, i, :] = feature_scaled.reshape(m, h * w)
    scalers.append(scaler)
x_train = x_train_reshaped.reshape(m, c, h, w)

scaler_y = MinMaxScaler()
y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()

shuffle_indices = np.random.permutation(len(x_train))
x_train = x_train[shuffle_indices]
y_train = y_train[shuffle_indices]


x_train = torch.tensor(x_train, dtype=torch.float32).to(device)
y_train = torch.tensor(y_train, dtype=torch.float32).to(device)

x_test_list = []
y_test_list = []
test_lon_list = []
test_lat_list = []
test_topo_center_list = []
for j in range(len(check)):
    center_lat = che_lat_index[j]
    center_lon = che_lon_index[j]
    start_lat = max(0, center_lat - 8)
    end_lat = min(len(free_15s.lat), center_lat + 8)
    start_lon = max(0, center_lon - 8)
    end_lon = min(len(free_15s.lon), center_lon + 8)
    free_values = free_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    vgg_values = vgg_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    nvd_values = nvd_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    evd_values = evd_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    topo_values = topo_25_1_15s.isel(lat=slice(start_lat, end_lat), lon=slice(start_lon, end_lon)).values
    lat_slice = free_15s.lat[start_lat:end_lat]
    lon_slice = free_15s.lon[start_lon:end_lon]
    lon_grid, lat_grid = np.meshgrid(lon_slice, lat_slice)
    dlon = lon_grid - check.values[j, 0]
    dlat = lat_grid - check.values[j, 1]
    center_idx = 8
    free_center = free_values[center_idx, center_idx]
    free_diff = free_values - free_center
    vgg_center = vgg_values[center_idx, center_idx]
    vgg_diff = vgg_values - vgg_center
    nvd_center = nvd_values[center_idx, center_idx]
    nvd_diff = nvd_values - nvd_center
    evd_center = evd_values[center_idx, center_idx]
    evd_diff = evd_values - evd_center
    topo_center = topo_values[center_idx, center_idx]
    test_topo_center_list.append(topo_center)
    topo_diff = topo_values - topo_center
    data_temp = np.array([dlon, dlat, free_diff, vgg_diff, nvd_diff, evd_diff, topo_diff])
    if data_temp.shape != (7, 16, 16):
        continue
    x_test_list.append(data_temp)
    y_raw = check.values[j, 2]
    y_diff = topo_center - y_raw
    y_test_list.append(y_diff)
    test_lon_list.append(check.values[j, 0])
    test_lat_list.append(check.values[j, 1])

x_test = np.array(x_test_list)
y_test = np.array(y_test_list)
test_topo_center = np.array(test_topo_center_list).reshape(-1, 1)

mt, ct, ht, wt = x_test.shape
x_test_reshaped = x_test.reshape(mt, ct, ht * wt)
for i in range(ct):
    test_feature_flat = x_test_reshaped[:, i, :].reshape(-1, 1)
    test_feature_scaled = scalers[i].transform(test_feature_flat)
    x_test_reshaped[:, i, :] = test_feature_scaled.reshape(mt, ht * wt)
x_test = x_test_reshaped.reshape(mt, ct, ht, wt)
y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

x_test = torch.tensor(x_test, dtype=torch.float32).to(device)
y_test = torch.tensor(y_test, dtype=torch.float32).to(device)

config = BertConfig.from_json_file('')
model = Model(config).to(device)
optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)

reduce_lr = ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=3, threshold=0.0001,
                              threshold_mode='rel', min_lr=1e-7)

batch_size =
train_loss_history = []
test_mse_history = []
best_mse = float('inf')
best_model = None
scaler = GradScaler()

# 训练循环
for epoch in range(200):
    model.train()
    epoch_loss = 0.0
    total_batches = len(x_train) // batch_size
    if total_batches == 0:
        total_batches = 1
    for i in range(0, len(x_train), batch_size):
        batch_x = x_train[i:i + batch_size].to(device, non_blocking=True)
        batch_y = y_train[i:i + batch_size].to(device, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)
        with autocast():
            loss, _ = model(batch_x, batch_y)
            loss = loss * 100.0
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        scaler.step(optimizer)
        scaler.update()
        epoch_loss += (loss.item() / 100.0)
        del batch_x, batch_y, loss
        torch.cuda.empty_cache()
    avg_loss = epoch_loss / total_batches
    train_loss_history.append(avg_loss)
    # 每轮评估
    model.eval()
    with torch.no_grad():
        y_pred = model(x_test, None)
        test_mse = mean_squared_error(y_test.cpu(), y_pred.cpu())
        test_mse_history.append(test_mse)
        if test_mse < best_mse:
            best_mse = test_mse
            best_model = model.state_dict()
        print(
            f"Epoch {epoch}, Loss: {avg_loss:.4f}, Test MSE: {test_mse:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}")
    reduce_lr.step(avg_loss)
    model.train()

if best_model is not None:
    model.load_state_dict(best_model)

with torch.no_grad():
    pre_depth_diff = model(x_test, None)
    pre_depth_diff_original = scaler_y.inverse_transform(pre_depth_diff.cpu().numpy())
    actual_depth_pre = test_topo_center - pre_depth_diff_original

lon_huatu = np.array(test_lon_list).reshape(-1, 1)
lat_huatu = np.array(test_lat_list).reshape(-1, 1)
depth_cnn = np.concatenate((lon_huatu, lat_huatu, actual_depth_pre), axis=1)

torch.save(model, '')

pygmt.surface(region="", spacing="1m", outgrid="", data=depth_cnn)

depth_case = xr.open_dataset("", engine="netcdf4")
depth_case.to_netcdf("")
